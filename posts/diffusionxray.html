<html lang="en-GB">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DiffusionXRay — Aryan Goyal</title>
    <meta name="description" content="DiffusionXRay: A Diffusion and GAN-Based Approach for Enhancing Digitally Reconstructed Chest Radiographs.">

    <link rel="stylesheet" type="text/css" media="all" href="../assets/stylesheets/main_free.css" />
    <link rel="stylesheet" type="text/css" media="all" href="../clarity/clarity.css" />
    <link href="../assets/fontawesome-free-6.6.0-web/css/all.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" media="all" href="../assets/stylesheets/personal.css" />

    <script src="../assets/scripts/navbar.js"></script>
    <style>
        .figure-container {
            margin: 2rem 0;
            text-align: center;
        }
        .figure-container img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        .figure-container.small img {
            max-width: 400px;
        }
        .figure-container.medium img {
            max-width: 600px;
        }
        .figure-container.large img {
            max-width: 900px;
        }
        .figure-caption {
            margin-top: 0.75rem;
            font-size: 0.9rem;
            color: #666;
            font-style: italic;
        }
        .results-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }
        .results-table th, .results-table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: center;
        }
        .results-table th {
            background-color: #f5f5f5;
            font-weight: 600;
        }
        .results-table tr:nth-child(even) {
            background-color: #fafafa;
        }
        .highlight-box {
            background: linear-gradient(135deg, #e8f4f8 0%, #f0f7fa 100%);
            border-left: 4px solid #2196F3;
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }
        .comparison-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }
        .comparison-item {
            text-align: center;
        }
        .comparison-item img {
            width: 100%;
            max-width: 280px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .comparison-item p {
            margin-top: 0.5rem;
            font-size: 0.85rem;
            color: #555;
            font-weight: 500;
        }
        .xray-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 0.75rem;
            margin: 1.5rem 0;
        }
        .xray-grid img {
            width: 100%;
            border-radius: 4px;
        }
        .xray-grid .label {
            font-size: 0.8rem;
            text-align: center;
            color: #666;
            margin-top: 0.25rem;
        }
    </style>
</head>

<body>
    <div class="container blog" id="first-content" style="background-color: #E0E4E6;">
        <div class="blog-title">
            <div class="blog-intro">
                <div>
                    <h1 class="title">DiffusionXRay</h1>
                    <p class="author">Aryan Goyal†, Ashish Mittal†, Pranav Rao, Manoj Tadepalli, Preetham Putha</p>
                    <p class="author" style="margin-top: 0.25rem;">DEMI Workshop @ MICCAI 2025 (Accepted)</p>
                    <p class="abstract">
                        A two-stage diffusion and GAN-based pipeline for enhancing digitally reconstructed chest radiographs (DRRs), preserving fine-grained clinical details like subtle lung nodules that traditional super-resolution methods miss.
                    </p>
                    <div class="cta-row">
                        <a href="../index.html#research" class="button icon">Back <i class="fa-solid fa-arrow-left"></i></a>
                        <a href="#" class="button icon">Paper <i class="fa-regular fa-file-pdf"></i></a>
                        <a href="#" class="button icon">Code <i class="fa-solid fa-code"></i></a>
                        <a href="../assets/figures/qure/Qure_Presentation.pdf" class="button icon">Slides <i class="fa-solid fa-file-powerpoint"></i></a>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container blog main first">
        <h1>The Problem: Enhancing DRRs for Clinical Use</h1>
        
        <p class="text">
            Early detection of lung cancer through chest X-rays (CXRs) is critical for patient outcomes, but subtle nodules are notoriously difficult to spot, and labeled datasets are scarce. <b>Digitally Reconstructed Radiographs (DRRs)</b>—synthetic X-rays generated by projecting CT volumes—offer a scalable solution for creating training data. However, DRRs suffer from significant quality issues: blur, loss of fine anatomical structures, and projection artifacts.
        </p>

        <div class="figure-container medium">
            <img src="../assets/figures/diffusionxray/drr.png" alt="DRR explanation diagram">
            <p class="figure-caption">Digitally Reconstructed Radiographs are created by projecting CT volumes, but the process introduces quality degradation.</p>
        </div>

        <p class="text">
            The challenge is clear: how do we enhance these DRRs to match real X-ray quality while preserving the clinically important details—especially subtle lung nodules that could indicate early-stage cancer?
        </p>

        <h2>Example DRR Images</h2>
        <p class="text">
            Here are examples of DRRs generated from CT scans. Notice the characteristic blur and lack of fine detail compared to real radiographs:
        </p>

        <div class="comparison-grid">
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/1.2.392.200036.9116.2.6.1.16.1613459502.1646799405.670204_axial_11_1024.png" alt="DRR Example 1">
                <p>DRR Sample 1</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/1.2.392.200036.9116.2.6.1.16.1613459502.1646799405.670204_axial_13_1024.png" alt="DRR Example 2">
                <p>DRR Sample 2</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/10399282707403106701579978291218_axial_16_1024.png" alt="DRR Example 3">
                <p>DRR Sample 3</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/12001905933555293595683974028768_axial_20_1024_PA.png" alt="DRR Example 4">
                <p>DRR Sample 4</p>
            </div>
        </div>
    </div>

    <div class="container blog main">
        <h1>Why Standard Super-Resolution Fails</h1>
        
        <p class="text">
            Our first instinct was to try established super-resolution methods. The idea seems straightforward: generate paired low-quality/high-quality data using standard degradations (like bicubic downsampling), then train a model to reverse the process. We tried state-of-the-art transformer-based methods including <b>SwinFIR</b> and <b>SwinIR</b>.
        </p>

        <p class="text">
            The results were disappointing. These methods are trained on natural image degradations that don't match the unique characteristics of DRR artifacts. The models produced over-smoothed outputs that lost critical fine-grained details.
        </p>

        <div class="figure-container large">
            <img src="../assets/figures/diffusionxray/2.25.10052321410400682055889766654257027327216846983274015370808_SwinFIR.png" alt="SwinFIR results">
            <p class="figure-caption">Results from SwinFIR: The transformer-based method fails to recover fine anatomical structures and introduces artifacts.</p>
        </div>

        <div class="highlight-box">
            <p class="text" style="margin: 0;">
                <b>Key Insight:</b> The fundamental problem is the <em>domain gap</em>. Standard degradation models (bicubic, Gaussian blur) don't capture the real characteristics of DRR artifacts. We need to learn the actual degradation distribution from real data.
            </p>
        </div>
    </div>

    <div class="container blog main">
        <h1>Our Approach: DiffusionXRay</h1>
        
        <p class="text">
            DiffusionXRay is a two-stage pipeline that addresses the domain gap by first learning realistic degradations, then training an enhancement model on properly paired data.
        </p>

        <h2>Stage 1: Learning Realistic Degradations</h2>
        
        <p class="text">
            Instead of assuming a degradation model, we learn it from data using two complementary approaches:
        </p>

        <h3>MUNIT-LQ: Unpaired Style Transfer</h3>
        <p class="text">
            <b>MUNIT</b> (Multimodal Unsupervised Image-to-Image Translation) disentangles images into content and style components. We use it to transfer the "style" of low-quality DRRs onto high-quality X-rays while preserving their anatomical content. This gives us realistic LQ-HQ pairs without requiring aligned training data.
        </p>

        <div class="figure-container medium">
            <img src="../assets/figures/diffusionxray/munit_results.png" alt="MUNIT-LQ results">
            <p class="figure-caption">MUNIT-LQ learns to transfer degradation characteristics from real DRRs to high-quality X-rays.</p>
        </div>

        <h3>DDPM-LQ: Diffusion-Based Degradation</h3>
        <p class="text">
            We also train a conditional <b>Denoising Diffusion Probabilistic Model (DDPM)</b> to learn the degradation distribution. The training proceeds in two sub-stages:
        </p>
        <ol>
            <li><p class="text"><b>Unconditional training</b> on real low-quality DRRs to learn the LQ distribution.</p></li>
            <li><p class="text"><b>Conditional fine-tuning</b> on HQ images to generate realistic paired LQ versions.</p></li>
        </ol>

        <h2>Stage 2: DDPM-HQ Enhancement</h2>
        
        <p class="text">
            With realistic paired data from Stage 1, we train a conditional DDPM enhancer. Unlike the degradation models that learn to add artifacts, DDPM-HQ learns to remove them while faithfully reconstructing fine anatomical details.
        </p>

        <p class="text">
            The diffusion framework is particularly well-suited for this task because it can model complex, multi-modal distributions—exactly what's needed when the "correct" enhancement isn't unique but depends on preserving different types of clinical findings.
        </p>
    </div>

    <div class="container blog main">
        <h1>Results</h1>
        
        <h2>Qualitative Comparison</h2>
        
        <p class="text">
            The following comparisons show enhancement results on test images. Each row shows: input (degraded), our DiffusionXRay result, and the reference high-quality image.
        </p>

        <h3>Sample 1</h3>
        <div class="comparison-grid">
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/input1.png" alt="Input 1">
                <p>Input (Low Quality)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/dfx1.png" alt="DiffusionXRay 1">
                <p>DiffusionXRay (Ours)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/ref1.png" alt="Reference 1">
                <p>Reference (HQ)</p>
            </div>
        </div>

        <h3>Sample 2</h3>
        <div class="comparison-grid">
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/input2.png" alt="Input 2">
                <p>Input (Low Quality)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/dfx2.png" alt="DiffusionXRay 2">
                <p>DiffusionXRay (Ours)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/ref2.png" alt="Reference 2">
                <p>Reference (HQ)</p>
            </div>
        </div>

        <h3>Sample 3</h3>
        <div class="comparison-grid">
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/input3.png" alt="Input 3">
                <p>Input (Low Quality)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/dfx3.png" alt="DiffusionXRay 3">
                <p>DiffusionXRay (Ours)</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/ref3.png" alt="Reference 3">
                <p>Reference (HQ)</p>
            </div>
        </div>

        <h2>Comparison with Baseline Methods</h2>
        
        <p class="text">
            Here we compare against baseline methods including bicubic interpolation and standard enhancement pipelines:
        </p>

        <div class="comparison-grid">
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/chxr1.png" alt="Baseline comparison 1">
                <p>Baseline Result</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/chx2.png" alt="Baseline comparison 2">
                <p>Baseline Result</p>
            </div>
            <div class="comparison-item">
                <img src="../assets/figures/diffusionxray/chx3.png" alt="Baseline comparison 3">
                <p>Baseline Result</p>
            </div>
        </div>

        <h2>Quantitative Results</h2>
        
        <table class="results-table">
            <thead>
                <tr>
                    <th>Method</th>
                    <th>PSNR ↑</th>
                    <th>SSIM ↑</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td colspan="3" style="background: #e8f4f8; font-weight: 600;">With MUNIT-LQ Generated Data</td>
                </tr>
                <tr>
                    <td>Bicubic-DDPM (Baseline)</td>
                    <td>20.08</td>
                    <td>0.83</td>
                </tr>
                <tr style="background: #d4edda;">
                    <td><b>DiffusionXRay (Ours)</b></td>
                    <td><b>27.50</b></td>
                    <td><b>0.92</b></td>
                </tr>
                <tr>
                    <td colspan="3" style="background: #e8f4f8; font-weight: 600;">With DDPM-LQ Generated Data</td>
                </tr>
                <tr>
                    <td>DDPM-LQ Baseline</td>
                    <td>19.85</td>
                    <td>0.78</td>
                </tr>
                <tr style="background: #d4edda;">
                    <td><b>DDPM-LQ (Ours)</b></td>
                    <td><b>22.21</b></td>
                    <td>0.78</td>
                </tr>
            </tbody>
        </table>

        <p class="text">
            Our method achieves a <b>+7.42 dB improvement in PSNR</b> and <b>+0.09 improvement in SSIM</b> over the bicubic baseline when using MUNIT-LQ for degradation synthesis.
        </p>

        <h2>Radiologist Evaluation</h2>
        
        <p class="text">
            Quantitative metrics only tell part of the story. We conducted a blinded evaluation with expert radiologists to assess clinical utility:
        </p>

        <table class="results-table">
            <thead>
                <tr>
                    <th>Evaluation Task</th>
                    <th>Question</th>
                    <th>DiffusionXRay</th>
                    <th>Baseline</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td rowspan="2"><b>Nodule Preservation</b></td>
                    <td>Nodule easily visible?</td>
                    <td style="background: #d4edda;"><b>100%</b></td>
                    <td>6.6%</td>
                </tr>
                <tr>
                    <td>Confusion with other structures?</td>
                    <td style="background: #d4edda;"><b>0%</b></td>
                    <td>30%</td>
                </tr>
                <tr>
                    <td rowspan="2"><b>Quality Assessment</b></td>
                    <td>Lung field clarity improved?</td>
                    <td style="background: #d4edda;"><b>100%</b></td>
                    <td>66.7%</td>
                </tr>
                <tr>
                    <td>Significant noise increase?</td>
                    <td>72.9%</td>
                    <td>25%</td>
                </tr>
            </tbody>
        </table>

        <div class="highlight-box">
            <p class="text" style="margin: 0;">
                <b>Clinical Significance:</b> 100% nodule visibility with 0% confusion represents a clinically meaningful improvement. The 72.9% perceived noise reflects a purposeful trade-off: we prioritize diagnostic conspicuity over smoothing, ensuring that subtle findings remain visible even if the image appears slightly noisier.
            </p>
        </div>
    </div>

    <div class="container blog main">
        <h1>Limitations & Future Directions</h1>
        
        <h2>Current Limitations</h2>
        <ul>
            <li><p class="text"><b>Computational cost:</b> Training diffusion models is expensive, and inference requires multiple denoising steps.</p></li>
            <li><p class="text"><b>DRR-specific:</b> The current model is trained specifically for DRR enhancement and may not generalize to other degradation types.</p></li>
        </ul>

        <h2>Future Work</h2>
        <ul>
            <li><p class="text"><b>Efficient architectures:</b> Exploring lighter backbones or knowledge distillation for faster inference.</p></li>
            <li><p class="text"><b>Broader applicability:</b> Extending to non-DRR scenarios such as portable X-rays and other imaging modalities.</p></li>
            <li><p class="text"><b>Downstream integration:</b> Evaluating impact on CAD systems for nodule detection.</p></li>
        </ul>
    </div>

    <div class="container blog main">
        <h1>Citation</h1>
<pre><code class="plaintext">@inproceedings{goyal2025diffusionxray,
  title     = {DiffusionXRay: A Diffusion and GAN-Based Approach for 
               Enhancing Digitally Reconstructed Chest Radiographs},
  author    = {Goyal, Aryan and Mittal, Ashish and Rao, Pranav and 
               Tadepalli, Manoj and Putha, Preetham},
  booktitle = {DEMI Workshop, MICCAI},
  year      = {2025},
}</code></pre>
    </div>

    <footer>
        <div class="container">
            <p>
                © <span id="year"></span> Aryan Goyal
            </p>
        </div>
    </footer>

    <script>
        document.getElementById('year').textContent = new Date().getFullYear();
    </script>
    <script src="../assets/scripts/main.js"></script>
</body>
</html>
